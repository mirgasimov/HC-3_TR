# -*- coding: utf-8 -*-
"""Миргасимов-Фролов(15.27В-ПИ12/24м)-ТВОРЧЕСКИЙ РЕЙТИНГ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HmZOucQqlpSuVmqQRNPA2PGlQg4vK_lE
"""

import keras  # Библиотека Keras для создания и обучения нейросетей
from keras.datasets import mnist  
from keras.models import Sequential  
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator 
from keras.optimizers import Adam  
import numpy as np  
import matplotlib.pyplot as plt 

(train_images, train_labels), (test_images, test_labels) = mnist.load_data() 

train_images = train_images.reshape((-1, 28, 28, 1)) / 255.0  # Изменение формы массива и нормализация
test_images = test_images.reshape((-1, 28, 28, 1)) / 255.0  # То же самое для тестового набора


validation_images = train_images[:1000]   
validation_labels = train_labels[:1000] 
train_images = train_images[1000:] 
train_labels = train_labels[1000:] 

# Создание генератора аугментации изображений
datagen = ImageDataGenerator(
    rotation_range=10,  # Вращение изображений до 10 градусов
    width_shift_range=0.1,  # Смещение по ширине до 10%
    height_shift_range=0.1,  # Смещение по высоте до 10%
    zoom_range=0.1  # Изменение масштаба до 10%
)
datagen.fit(train_images)  # Адаптация генератора к тренировочным данным

# Создание сверточной нейросети
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),  # Сверточный слой (32 фильтра 3x3)
    MaxPooling2D(2,2),  # Слой подвыборки (уменьшает размерность в 2 раза)
    Conv2D(64, (3,3), activation='relu'),  # Еще один сверточный слой (64 фильтра 3x3)
    MaxPooling2D(2,2),  # Еще один слой подвыборки
    Flatten(),  
    Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),  # Полносвязный слой (256 нейронов)
    Dropout(0.3),  # Исключение 30% нейронов для предотвращения переобучения
    Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),  # Еще один полносвязный слой (128 нейронов)
    Dropout(0.3),  # Еще один слой Dropout (30%)
    Dense(10, activation='softmax') 
])


model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',  
              metrics=['accuracy'])  # Метрика точности

# Обучение модели
history = model.fit(
    datagen.flow(train_images, train_labels, batch_size=128),  # Обучение с использованием аугментации
    epochs=20,  
    validation_data=(validation_images, validation_labels) 
)


test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)  
print('\nТочность на проверочных данных:', test_acc)


def show_loss(history):
    loss = history.history['loss'] 
    val_loss = history.history['val_loss'] 
    epochs = range(1, len(loss) + 1)  

    
    plt.plot(epochs, loss, 'bo', label='Training loss') 
    plt.plot(epochs, val_loss, 'b', label='Validation loss') 
    plt.xlabel('Epochs')  
    plt.ylabel('Loss') 
    plt.legend(['Train', 'Test'], loc='upper left')  
    plt.show()  

   
    plt.plot(history.history['accuracy'])  
    plt.plot(history.history['val_accuracy']) 
    plt.title('Model accuracy')  
    plt.ylabel('Accuracy')  
    plt.xlabel('Epoch')  
    plt.legend(['Train', 'Test'], loc='upper left') 
    plt.show()


show_loss(history)
